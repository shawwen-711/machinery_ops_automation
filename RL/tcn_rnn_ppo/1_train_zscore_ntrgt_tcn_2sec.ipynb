{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6643060b",
   "metadata": {
    "papermill": {
     "duration": 0.003223,
     "end_time": "2025-03-04T09:25:05.236948",
     "exception": false,
     "start_time": "2025-03-04T09:25:05.233725",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Data Loading & Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5d42dc",
   "metadata": {
    "papermill": {
     "duration": 0.002134,
     "end_time": "2025-03-04T09:25:05.241328",
     "exception": false,
     "start_time": "2025-03-04T09:25:05.239194",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### _Loading Data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2448fbbd",
   "metadata": {
    "papermill": {
     "duration": 0.017715,
     "end_time": "2025-03-04T09:25:14.616152",
     "exception": false,
     "start_time": "2025-03-04T09:25:14.598437",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/wensx/repos/IFG_DFO/RL/src/FY25_Oct/Shaw/rl_v6\n",
      "(6039, 149)\n"
     ]
    }
   ],
   "source": [
    "import os, sys, pickle\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "\n",
    "cwd = os.getcwd()\n",
    "print(cwd)\n",
    "\n",
    "sys.path.append(os.path.join(cwd, 'utils'))\n",
    "sys.path.append(os.path.join(cwd, 'tcn_models'))\n",
    "\n",
    "# pd.set_option('display.max_rows', None)\n",
    "from utils import tools\n",
    "from tcn_models import TCNMultiTarget\n",
    "\n",
    "model_dir = os.path.join(cwd[:cwd.find('IFG_DFO/RL')], 'IFG_DFO/RL/models/ifg_v5/rl_v6_multi_trgts_tcn_model')\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "# retrieve merged data from ifg_v5\n",
    "ifg_ver = 5\n",
    "# print (os.listdir(os.path.join(tools.get_ifg_dir(ifg_ver), 'merged')))\n",
    "filename = \"merged/inner_hist_wip_merge_2sec.csv\"\n",
    "\n",
    "data_df = tools.get_ifg_file (ifg_ver, filename)\n",
    "print(data_df.shape)\n",
    "\n",
    "### -------------------------------------------------------------------\n",
    "# Target Variables by Group\n",
    "# -------------------------------------------------------------------\n",
    "ctrl_vars = ['Dryer/process/hmi_pide_moisture_comp_cv',  # dryer_temp_cv\n",
    "             'Dryer/vfds/hmi_vfd_01/status/act_freq',        # dryer_feed_rate\n",
    "             'Dryer/vfds/hmi_vfd_03/status/act_freq']        # supply_feed_rate\n",
    "\n",
    "hist_p1_vars = ['Dryer/data/hmi_mt_2_reading_1_real', 'Dryer/data/hmi_mt_2_temperature_1_real']\n",
    "\n",
    "hist_p2_vars = ['Dryer/data/hmi_mt_1_reading_1_real', 'Dryer/data/hmi_mt_1_temperature_1_real']\n",
    "\n",
    "hist_p3_vars = ['Dryer/data/vibe_temp_f', 'Dryer/data/vibe_exhaust_temp_f',]\n",
    "\n",
    "hist_p4_vars = ['Dryer/process/hmi_pide_moisture_comp_pv', 'Dryer/process/hmi_pide_moisture_comp_sp',]\n",
    "                \n",
    "wip_p1_vars = ['Distance', 'Particles']\n",
    "\n",
    "wip_p2_vars = ['X50', 'Xc', 'D01', 'D05', 'D10', 'D20', 'D25', 'D50', 'D75', 'D80', 'D90', 'D95', 'D99']\n",
    "\n",
    "wip_p3_vars = ['3 mm', '3.5 mm', '4 mm', '5 mm', '6 mm', '7 mm', '8 mm', '9 mm', '10 mm', '11 mm', '12 mm', '13 mm', '14 mm']\n",
    "\n",
    "\n",
    "### -------------------------------------------------------------------\n",
    "# Variable Preparation\n",
    "# -------------------------------------------------------------------\n",
    "group_names = ['hist_p1', 'hist_p2', 'hist_p3', 'hist_p4', 'wip_p1', 'wip_p2', 'wip_p3']\n",
    "target_vars_dict = {'hist_p1': hist_p1_vars, 'hist_p2': hist_p2_vars, 'hist_p3': hist_p3_vars, 'hist_p4': hist_p4_vars,\n",
    "                    'wip_p1': wip_p1_vars, 'wip_p2': wip_p2_vars, 'wip_p3': wip_p3_vars,}\n",
    "\n",
    "state_vars = [target_vars_dict[grp] for grp in group_names]\n",
    "state_vars = list(chain.from_iterable(state_vars))\n",
    "# state_vars\n",
    "\n",
    "all_vars = ctrl_vars + state_vars\n",
    "all_features = data_df[all_vars].to_numpy(dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dd7854b9",
   "metadata": {
    "papermill": {
     "duration": 0.017715,
     "end_time": "2025-03-04T09:25:14.616152",
     "exception": false,
     "start_time": "2025-03-04T09:25:14.598437",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target group name: hist_p1\n",
      "['Dryer/data/hmi_mt_2_reading_1_real', 'Dryer/data/hmi_mt_2_temperature_1_real']\n"
     ]
    }
   ],
   "source": [
    "group_idx = 0\n",
    "\n",
    "target_vars = target_vars_dict[group_names[group_idx]] \n",
    "print(f\"target group name: {group_names[group_idx]}\\n{target_vars}\")\n",
    "targets = data_df[target_vars].to_numpy(dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340e4243",
   "metadata": {
    "papermill": {
     "duration": 0.002352,
     "end_time": "2025-03-04T09:25:14.621556",
     "exception": false,
     "start_time": "2025-03-04T09:25:14.619204",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5184044e",
   "metadata": {
    "papermill": {
     "duration": 0.002289,
     "end_time": "2025-03-04T09:25:14.626077",
     "exception": false,
     "start_time": "2025-03-04T09:25:14.623788",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### _Preprocess & Scaling_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ceb61c7",
   "metadata": {
    "papermill": {
     "duration": 2.519656,
     "end_time": "2025-03-04T09:25:17.147984",
     "exception": false,
     "start_time": "2025-03-04T09:25:14.628328",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Create Fixed-Length Sequences with a Sliding Window\n",
    "# -------------------------------------------------------------------\n",
    "def create_sequences(data, targets, seq_length):\n",
    "    # data: shape (total_timesteps, num_features)\n",
    "    # targets: shape (total_timesteps, num_targets)\n",
    "    sequences = [data[i:i+seq_length].T for i in range(len(data) - seq_length)]\n",
    "    labels = [targets[i+seq_length] for i in range(len(data) - seq_length)]\n",
    "    return np.array(sequences), np.array(labels)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# MinMax Scaling Functions\n",
    "# -------------------------------------------------------------------\n",
    "def minmax_scale_features(x, scaler=None, feature_range=(0,1)):\n",
    "    \"\"\"\n",
    "    Scales features using MinMax scaling.\n",
    "    x: numpy array of shape (samples, num_features, seq_length)\n",
    "    Returns: scaled x and the fitted scaler.\n",
    "    \"\"\"\n",
    "    samples, num_features, seq_length = x.shape\n",
    "    # Reshape to (samples*seq_length, num_features)\n",
    "    x_reshaped = x.transpose(0, 2, 1).reshape(-1, num_features)\n",
    "    if scaler is None:\n",
    "        scaler = MinMaxScaler(feature_range=feature_range)\n",
    "        x_scaled = scaler.fit_transform(x_reshaped)\n",
    "    else:\n",
    "        x_scaled = scaler.transform(x_reshaped)\n",
    "    # Reshape back to (samples, seq_length, num_features) then transpose to (samples, num_features, seq_length)\n",
    "    x_scaled = x_scaled.reshape(samples, seq_length, num_features).transpose(0, 2, 1)\n",
    "    return x_scaled, scaler\n",
    "\n",
    "def minmax_scale_targets(y, scaler=None, feature_range=(0,1)):\n",
    "    \"\"\"\n",
    "    Scales targets using MinMax scaling.\n",
    "    y: numpy array of shape (samples, num_targets)\n",
    "    Returns: scaled y and the fitted scaler.\n",
    "    \"\"\"\n",
    "    if scaler is None:\n",
    "        scaler = MinMaxScaler(feature_range=feature_range)\n",
    "        y_scaled = scaler.fit_transform(y)\n",
    "    else:\n",
    "        y_scaled = scaler.transform(y)\n",
    "    return y_scaled, scaler\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Standard Scaling Functions\n",
    "# -------------------------------------------------------------------\n",
    "def standard_scale_features(x, scaler=None):\n",
    "    \"\"\"\n",
    "    Scales features using Standard (z-score) scaling.\n",
    "    x: numpy array of shape (samples, num_features, seq_length)\n",
    "    Returns: scaled x and the fitted scaler.\n",
    "    \"\"\"\n",
    "    samples, num_features, seq_length = x.shape\n",
    "    # Reshape to (samples*seq_length, num_features)\n",
    "    x_reshaped = x.transpose(0, 2, 1).reshape(-1, num_features)\n",
    "    if scaler is None:\n",
    "        scaler = StandardScaler()\n",
    "        x_scaled = scaler.fit_transform(x_reshaped)\n",
    "    else:\n",
    "        x_scaled = scaler.transform(x_reshaped)\n",
    "    # Reshape back to (samples, seq_length, num_features) then transpose to (samples, num_features, seq_length)\n",
    "    x_scaled = x_scaled.reshape(samples, seq_length, num_features).transpose(0, 2, 1)\n",
    "    return x_scaled, scaler\n",
    "\n",
    "def standard_scale_targets(y, scaler=None):\n",
    "    \"\"\"\n",
    "    Scales targets using Standard (z-score) scaling.\n",
    "    y: numpy array of shape (samples, num_targets)\n",
    "    Returns: scaled y and the fitted scaler.\n",
    "    \"\"\"\n",
    "    if scaler is None:\n",
    "        scaler = StandardScaler()\n",
    "        y_scaled = scaler.fit_transform(y)\n",
    "    else:\n",
    "        y_scaled = scaler.transform(y)\n",
    "    return y_scaled, scaler\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Dataset for Multi-Target Training\n",
    "# -------------------------------------------------------------------\n",
    "class MultiTargetDataset(Dataset):\n",
    "    def __init__(self, features, targets):\n",
    "        \"\"\"\n",
    "        features: numpy array of shape (num_samples, total_vars, seq_length)\n",
    "        targets: numpy array of shape (num_samples, num_state_vars)\n",
    "        \"\"\"\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.targets = torch.tensor(targets, dtype=torch.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.targets[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8bb1950d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 1.356391,
     "end_time": "2025-03-04T09:25:18.507127",
     "exception": false,
     "start_time": "2025-03-04T09:25:17.150736",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------\n",
    "# Data Preparation\n",
    "# -------------------------------------------------------------------\n",
    "max_window = 510  # fixed sequence length (e.g., for 2sec intervals)\n",
    "\n",
    "# Assume all_features and targets are defined elsewhere.\n",
    "# all_features: numpy array of shape (total_timesteps, len(all_vars))\n",
    "# targets: numpy array of shape (total_timesteps, num_state_vars)\n",
    "\n",
    "# Create sequences:\n",
    "# x_seq shape: (samples, len(all_vars), seq_length)\n",
    "# y_seq shape: (samples, num_state_vars)\n",
    "x_seq, y_seq = create_sequences(all_features, targets, max_window)\n",
    "\n",
    "# Split data while preserving time order\n",
    "x_train, x_val, y_train, y_val = train_test_split(\n",
    "    x_seq, y_seq, test_size=0.1, shuffle=False, random_state=42\n",
    ")\n",
    "\n",
    "# # Apply MinMax scaling to the features and targets.\n",
    "# # Note: We fit the scaler on the training data and then transform both training and validation sets.\n",
    "# x_train_scaled, feat_scaler = minmax_scale_features(x_train)\n",
    "# y_train_scaled, targ_scaler = minmax_scale_targets(y_train)\n",
    "# x_val_scaled, _ = minmax_scale_features(x_val, scaler=feat_scaler)\n",
    "# y_val_scaled, _ = minmax_scale_targets(y_val, scaler=targ_scaler)\n",
    "\n",
    "\n",
    "x_train_scaled, feat_scaler = standard_scale_features(x_train)\n",
    "y_train_scaled, targ_scaler = standard_scale_targets(y_train)\n",
    "x_val_scaled, _ = standard_scale_features(x_val, scaler=feat_scaler)\n",
    "y_val_scaled, _ = standard_scale_targets(y_val, scaler=targ_scaler)\n",
    "\n",
    "# Create dataset objects using the scaled data\n",
    "train_dataset = MultiTargetDataset(x_train_scaled, y_train_scaled)\n",
    "val_dataset = MultiTargetDataset(x_val_scaled, y_val_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68eca9da",
   "metadata": {
    "papermill": {
     "duration": 0.002489,
     "end_time": "2025-03-04T09:25:18.513181",
     "exception": false,
     "start_time": "2025-03-04T09:25:18.510692",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### _Save Info for RL Training_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9a6f622",
   "metadata": {
    "papermill": {
     "duration": 0.018635,
     "end_time": "2025-03-04T09:25:18.534360",
     "exception": false,
     "start_time": "2025-03-04T09:25:18.515725",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_window: 510\n"
     ]
    }
   ],
   "source": [
    "print(\"max_window:\", max_window)\n",
    "\n",
    "rl_setup_info = {\n",
    "    'data': data_df,\n",
    "    'ctrl_vars': ctrl_vars,\n",
    "    'state_vars': state_vars,\n",
    "    'group_name': group_names[group_idx],\n",
    "    'trgt_vars': target_vars,\n",
    "    'seq_len': max_window,\n",
    "    'ftr_scaler': feat_scaler,\n",
    "    'trgt_scaler': targ_scaler,\n",
    "}\n",
    "\n",
    "# save_dir = tools.get_ifg_dir(ifg_ver)\n",
    "# save_dir = os.path.join(save_dir, 'train', 'v7')\n",
    "# if not os.path.isdir(save_dir):\n",
    "#     os.makedirs(save_dir)\n",
    "#     assert os.path.isdir(save_dir), 'Save directory not created!'\n",
    "\n",
    "# fn = f\"{group_names[group_idx]}_tcn_seqlen{max_window}_inner_merge_2sec_standard.pkl\"\n",
    "# with open(os.path.join(save_dir, fn), 'wb') as file:\n",
    "#     pickle.dump(rl_setup_info, file)\n",
    "#     print(f\"saved to: {os.path.join(save_dir, fn)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccfa398",
   "metadata": {
    "papermill": {
     "duration": 0.002516,
     "end_time": "2025-03-04T09:25:18.539491",
     "exception": false,
     "start_time": "2025-03-04T09:25:18.536975",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce42cd27",
   "metadata": {
    "papermill": {
     "duration": 0.002499,
     "end_time": "2025-03-04T09:25:18.544536",
     "exception": false,
     "start_time": "2025-03-04T09:25:18.542037",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### _Train Multi-Target TCN_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b932ebb5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 1373.852545,
     "end_time": "2025-03-04T09:48:12.399696",
     "exception": false,
     "start_time": "2025-03-04T09:25:18.547151",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch [3/150] - Train Loss: 0.4548 - Val Loss: 7.8315\n",
      "Epoch [4/150] - Train Loss: 0.3165 - Val Loss: 6.0636\n",
      "Epoch [5/150] - Train Loss: 0.2584 - Val Loss: 7.5321\n",
      "Epoch [6/150] - Train Loss: 0.2259 - Val Loss: 6.8648\n",
      "Epoch [7/150] - Train Loss: 0.2009 - Val Loss: 7.5067\n",
      "Epoch [8/150] - Train Loss: 0.1882 - Val Loss: 7.3725\n",
      "Epoch [9/150] - Train Loss: 0.1787 - Val Loss: 6.5456\n",
      "Epoch [10/150] - Train Loss: 0.1653 - Val Loss: 5.7846\n",
      "Epoch [11/150] - Train Loss: 0.1558 - Val Loss: 4.8715\n",
      "Epoch [12/150] - Train Loss: 0.1522 - Val Loss: 3.7300\n",
      "**************** Flagged for Saving! ******************\n",
      "Epoch [13/150] - Train Loss: 0.1727 - Val Loss: 3.9377\n",
      "Epoch [14/150] - Train Loss: 0.1891 - Val Loss: 4.1406\n",
      "Epoch [15/150] - Train Loss: 0.1806 - Val Loss: 3.6850\n",
      "**************** Flagged for Saving! ******************\n",
      "Epoch [16/150] - Train Loss: 0.1892 - Val Loss: 2.8488\n",
      "**************** Flagged for Saving! ******************\n",
      "Epoch [17/150] - Train Loss: 0.2017 - Val Loss: 3.1363\n",
      "Epoch [18/150] - Train Loss: 0.2359 - Val Loss: 2.7604\n",
      "**************** Flagged for Saving! ******************\n",
      "Epoch [19/150] - Train Loss: 0.2112 - Val Loss: 4.0186\n",
      "Epoch [20/150] - Train Loss: 0.2164 - Val Loss: 4.6998\n",
      "Epoch [21/150] - Train Loss: 0.2382 - Val Loss: 5.1446\n",
      "Epoch [22/150] - Train Loss: 0.2342 - Val Loss: 4.7761\n",
      "Epoch [23/150] - Train Loss: 0.2172 - Val Loss: 5.1496\n",
      "Epoch [24/150] - Train Loss: 0.2155 - Val Loss: 4.4244\n",
      "Epoch [25/150] - Train Loss: 0.1871 - Val Loss: 3.7050\n",
      "Epoch [26/150] - Train Loss: 0.1906 - Val Loss: 2.4626\n",
      "**************** Flagged for Saving! ******************\n",
      "Epoch [27/150] - Train Loss: 0.1768 - Val Loss: 3.6175\n",
      "Epoch [28/150] - Train Loss: 0.1931 - Val Loss: 2.6685\n",
      "Epoch [29/150] - Train Loss: 0.1740 - Val Loss: 3.2544\n",
      "Epoch [30/150] - Train Loss: 0.1486 - Val Loss: 2.4109\n",
      "**************** Flagged for Saving! ******************\n",
      "Epoch [31/150] - Train Loss: 0.1417 - Val Loss: 2.5192\n",
      "Epoch [32/150] - Train Loss: 0.1501 - Val Loss: 2.5748\n",
      "Epoch [33/150] - Train Loss: 0.1454 - Val Loss: 2.1524\n",
      "**************** Flagged for Saving! ******************\n",
      "Epoch [34/150] - Train Loss: 0.1346 - Val Loss: 2.1496\n",
      "**************** Flagged for Saving! ******************\n",
      "Epoch [35/150] - Train Loss: 0.1303 - Val Loss: 3.1668\n",
      "Epoch [36/150] - Train Loss: 0.1153 - Val Loss: 2.2770\n",
      "Epoch [37/150] - Train Loss: 0.1276 - Val Loss: 2.6079\n",
      "Epoch [38/150] - Train Loss: 0.1312 - Val Loss: 2.1196\n",
      "**************** Flagged for Saving! ******************\n",
      "Epoch [39/150] - Train Loss: 0.1250 - Val Loss: 2.7945\n",
      "Epoch [40/150] - Train Loss: 0.1337 - Val Loss: 2.1908\n",
      "Epoch [41/150] - Train Loss: 0.1489 - Val Loss: 2.5498\n",
      "Epoch [42/150] - Train Loss: 0.1592 - Val Loss: 2.1302\n",
      "Epoch [43/150] - Train Loss: 0.1426 - Val Loss: 2.2305\n",
      "Epoch [44/150] - Train Loss: 0.1387 - Val Loss: 3.4533\n",
      "Epoch [45/150] - Train Loss: 0.1318 - Val Loss: 2.0950\n",
      "**************** Flagged for Saving! ******************\n",
      "Epoch [46/150] - Train Loss: 0.1500 - Val Loss: 3.8100\n",
      "Epoch [47/150] - Train Loss: 0.1470 - Val Loss: 2.9420\n",
      "Epoch [48/150] - Train Loss: 0.1396 - Val Loss: 2.0199\n",
      "**************** Flagged for Saving! ******************\n",
      "Epoch [49/150] - Train Loss: 0.1329 - Val Loss: 2.0853\n",
      "Epoch [50/150] - Train Loss: 0.1139 - Val Loss: 1.9309\n",
      "**************** Flagged for Saving! ******************\n",
      "Epoch [51/150] - Train Loss: 0.1078 - Val Loss: 2.3216\n",
      "Epoch [52/150] - Train Loss: 0.1237 - Val Loss: 2.3779\n",
      "Epoch [53/150] - Train Loss: 0.1152 - Val Loss: 1.2486\n",
      "**************** Flagged for Saving! ******************\n",
      "Epoch [54/150] - Train Loss: 0.1123 - Val Loss: 1.7148\n",
      "Epoch [55/150] - Train Loss: 0.1096 - Val Loss: 1.7372\n",
      "Epoch [56/150] - Train Loss: 0.1039 - Val Loss: 1.7134\n",
      "Epoch [57/150] - Train Loss: 0.1053 - Val Loss: 3.7193\n",
      "Epoch [58/150] - Train Loss: 0.1047 - Val Loss: 1.7717\n",
      "Epoch [59/150] - Train Loss: 0.1108 - Val Loss: 3.5597\n",
      "Epoch [60/150] - Train Loss: 0.1147 - Val Loss: 1.4532\n",
      "Epoch [61/150] - Train Loss: 0.1144 - Val Loss: 3.2381\n",
      "Epoch [62/150] - Train Loss: 0.1207 - Val Loss: 2.9957\n",
      "Epoch [63/150] - Train Loss: 0.1232 - Val Loss: 1.9783\n",
      "Epoch [64/150] - Train Loss: 0.1203 - Val Loss: 2.6557\n",
      "Epoch [65/150] - Train Loss: 0.1120 - Val Loss: 2.2226\n",
      "Epoch [66/150] - Train Loss: 0.1013 - Val Loss: 2.3199\n",
      "Epoch [67/150] - Train Loss: 0.1061 - Val Loss: 2.5471\n",
      "Epoch [68/150] - Train Loss: 0.1049 - Val Loss: 2.1568\n",
      "Epoch [69/150] - Train Loss: 0.1009 - Val Loss: 2.1155\n",
      "Epoch [70/150] - Train Loss: 0.1004 - Val Loss: 1.5118\n",
      "Epoch [71/150] - Train Loss: 0.0985 - Val Loss: 2.1121\n",
      "Epoch [72/150] - Train Loss: 0.0943 - Val Loss: 1.3110\n",
      "Epoch [73/150] - Train Loss: 0.0883 - Val Loss: 1.7700\n",
      "Epoch [74/150] - Train Loss: 0.0915 - Val Loss: 1.4420\n",
      "Epoch [75/150] - Train Loss: 0.0989 - Val Loss: 2.8131\n",
      "Epoch [76/150] - Train Loss: 0.0927 - Val Loss: 1.6073\n",
      "Epoch [77/150] - Train Loss: 0.1021 - Val Loss: 2.5334\n",
      "Epoch [78/150] - Train Loss: 0.0925 - Val Loss: 1.0595\n",
      "**************** Flagged for Saving! ******************\n",
      "Epoch [79/150] - Train Loss: 0.0981 - Val Loss: 2.3264\n",
      "Epoch [80/150] - Train Loss: 0.1128 - Val Loss: 1.1758\n",
      "Epoch [81/150] - Train Loss: 0.1009 - Val Loss: 1.7662\n",
      "Epoch [82/150] - Train Loss: 0.0949 - Val Loss: 2.2374\n",
      "Epoch [83/150] - Train Loss: 0.0942 - Val Loss: 2.7957\n",
      "Epoch [84/150] - Train Loss: 0.0858 - Val Loss: 1.7501\n",
      "Epoch [85/150] - Train Loss: 0.0907 - Val Loss: 1.7976\n",
      "Epoch [86/150] - Train Loss: 0.0965 - Val Loss: 1.5670\n",
      "Epoch [87/150] - Train Loss: 0.0815 - Val Loss: 1.7137\n",
      "Epoch [88/150] - Train Loss: 0.0773 - Val Loss: 1.8886\n",
      "Epoch [89/150] - Train Loss: 0.0857 - Val Loss: 1.7778\n",
      "Epoch [90/150] - Train Loss: 0.0844 - Val Loss: 2.1071\n",
      "Epoch [91/150] - Train Loss: 0.0819 - Val Loss: 1.3845\n",
      "Epoch [92/150] - Train Loss: 0.0896 - Val Loss: 1.5923\n",
      "Epoch [93/150] - Train Loss: 0.0872 - Val Loss: 1.0818\n",
      "Epoch [94/150] - Train Loss: 0.0827 - Val Loss: 1.4927\n",
      "Epoch [95/150] - Train Loss: 0.0800 - Val Loss: 1.8093\n",
      "Epoch [96/150] - Train Loss: 0.0807 - Val Loss: 1.7708\n",
      "Epoch [97/150] - Train Loss: 0.0789 - Val Loss: 1.4799\n",
      "Epoch [98/150] - Train Loss: 0.0822 - Val Loss: 1.6903\n",
      "Epoch [99/150] - Train Loss: 0.0750 - Val Loss: 1.4424\n",
      "Epoch [100/150] - Train Loss: 0.0801 - Val Loss: 2.1801\n",
      "Epoch [101/150] - Train Loss: 0.0790 - Val Loss: 2.2742\n",
      "Epoch [102/150] - Train Loss: 0.0830 - Val Loss: 2.1820\n",
      "Epoch [103/150] - Train Loss: 0.0836 - Val Loss: 1.7904\n",
      "Epoch [104/150] - Train Loss: 0.0804 - Val Loss: 2.0212\n",
      "Epoch [105/150] - Train Loss: 0.0744 - Val Loss: 1.2870\n",
      "Epoch [106/150] - Train Loss: 0.0771 - Val Loss: 1.4855\n",
      "Epoch [107/150] - Train Loss: 0.0695 - Val Loss: 2.0658\n",
      "Epoch [108/150] - Train Loss: 0.0755 - Val Loss: 1.7146\n",
      "Epoch [109/150] - Train Loss: 0.0768 - Val Loss: 1.5928\n",
      "Epoch [110/150] - Train Loss: 0.0751 - Val Loss: 1.8517\n",
      "Epoch [111/150] - Train Loss: 0.0740 - Val Loss: 1.7489\n",
      "Epoch [112/150] - Train Loss: 0.0778 - Val Loss: 1.8476\n",
      "Epoch [113/150] - Train Loss: 0.0716 - Val Loss: 1.7478\n",
      "Epoch [114/150] - Train Loss: 0.0799 - Val Loss: 1.5094\n",
      "Epoch [115/150] - Train Loss: 0.0746 - Val Loss: 1.4693\n",
      "Epoch [116/150] - Train Loss: 0.0772 - Val Loss: 2.0303\n",
      "Epoch [117/150] - Train Loss: 0.0724 - Val Loss: 1.6540\n",
      "Epoch [118/150] - Train Loss: 0.0670 - Val Loss: 2.2852\n",
      "Epoch [119/150] - Train Loss: 0.0654 - Val Loss: 1.4840\n",
      "Epoch [120/150] - Train Loss: 0.0673 - Val Loss: 1.3681\n",
      "Epoch [121/150] - Train Loss: 0.0677 - Val Loss: 1.1744\n",
      "Epoch [122/150] - Train Loss: 0.0638 - Val Loss: 1.2524\n",
      "Epoch [123/150] - Train Loss: 0.0623 - Val Loss: 1.5244\n",
      "Epoch [124/150] - Train Loss: 0.0682 - Val Loss: 1.9186\n",
      "Epoch [125/150] - Train Loss: 0.0720 - Val Loss: 1.4276\n",
      "Epoch [126/150] - Train Loss: 0.0680 - Val Loss: 1.3017\n",
      "Epoch [127/150] - Train Loss: 0.0645 - Val Loss: 1.1365\n",
      "Epoch [128/150] - Train Loss: 0.0605 - Val Loss: 1.7560\n",
      "Epoch [129/150] - Train Loss: 0.0592 - Val Loss: 1.7948\n",
      "Epoch [130/150] - Train Loss: 0.0617 - Val Loss: 1.4181\n",
      "Epoch [131/150] - Train Loss: 0.0621 - Val Loss: 1.1527\n",
      "Epoch [132/150] - Train Loss: 0.0620 - Val Loss: 1.1858\n",
      "Epoch [133/150] - Train Loss: 0.0630 - Val Loss: 1.2030\n",
      "Epoch [134/150] - Train Loss: 0.0628 - Val Loss: 1.9730\n",
      "Epoch [135/150] - Train Loss: 0.0616 - Val Loss: 1.8235\n",
      "Epoch [136/150] - Train Loss: 0.0672 - Val Loss: 1.4020\n",
      "Epoch [137/150] - Train Loss: 0.0613 - Val Loss: 1.0538\n",
      "**************** Flagged for Saving! ******************\n",
      "Epoch [138/150] - Train Loss: 0.0616 - Val Loss: 1.3314\n",
      "Epoch [139/150] - Train Loss: 0.0580 - Val Loss: 1.3202\n",
      "Epoch [140/150] - Train Loss: 0.0588 - Val Loss: 1.4389\n",
      "Epoch [141/150] - Train Loss: 0.0634 - Val Loss: 1.2565\n",
      "Epoch [142/150] - Train Loss: 0.0640 - Val Loss: 1.1027\n",
      "Epoch [143/150] - Train Loss: 0.0669 - Val Loss: 1.1793\n",
      "Epoch [144/150] - Train Loss: 0.0656 - Val Loss: 1.1975\n",
      "Epoch [145/150] - Train Loss: 0.0629 - Val Loss: 1.3249\n",
      "Epoch [146/150] - Train Loss: 0.0637 - Val Loss: 1.7624\n",
      "Epoch [147/150] - Train Loss: 0.0622 - Val Loss: 1.1894\n",
      "Epoch [148/150] - Train Loss: 0.0623 - Val Loss: 1.3818\n",
      "Epoch [149/150] - Train Loss: 0.0578 - Val Loss: 1.0199\n",
      "**************** Flagged for Saving! ******************\n",
      "Epoch [150/150] - Train Loss: 0.0555 - Val Loss: 1.6038\n",
      "Training completed!\n",
      "Best model saved to: /home/wensx/repos/IFG_DFO/RL/models/ifg_v5/rl_v4_multi_trgts_tcn_model/hist_p2_standard_rf510_state_dict.pt\n",
      "Training finished!\n"
     ]
    }
   ],
   "source": [
    "# ### -------------------------------------------------------------------\n",
    "# # Training Loop for TCNMultiTarget Model\n",
    "# # -------------------------------------------------------------------\n",
    "# # weight_decay = 5e-3 -> too strong of a regularization\n",
    "# # best at weight_decay = 1e-3\n",
    "# def train_model(model, train_loader, val_loader, model_path, num_epochs=10, lr=1e-4, weight_decay=5e-4, device=\"cuda\"):\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "#     loss_fn = nn.MSELoss()\n",
    "#     model.to(device)\n",
    "\n",
    "#     best_train_loss = float('inf')\n",
    "#     best_val_loss = float('inf')\n",
    "#     best_overall_loss = float('inf')\n",
    "#     best_model_state = None\n",
    "\n",
    "#     for epoch in range(num_epochs):\n",
    "#         is_best_train_loss = False\n",
    "#         is_best_val_loss = False\n",
    "        \n",
    "#         model.train()\n",
    "#         total_train_loss = 0.0\n",
    "#         for batch in train_loader:\n",
    "#             x, y = [b.to(device) for b in batch]  # x: (batch, len(all_vars), seq_length), y: (batch, num_state_vars)\n",
    "#             optimizer.zero_grad()\n",
    "#             outputs = model(x)\n",
    "#             # Concatenate predictions from each target to form a tensor of shape (batch, num_state_vars)\n",
    "#             preds = torch.cat([outputs[target] for target in model.target_vars], dim=1)\n",
    "#             loss = loss_fn(preds, y)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             total_train_loss += loss.item()\n",
    "#         avg_train_loss = total_train_loss / len(train_loader)\n",
    "        \n",
    "#         is_best_train_loss = avg_train_loss < best_train_loss\n",
    "#         if is_best_train_loss:\n",
    "#             best_train_loss = avg_train_loss\n",
    "\n",
    "#         model.eval()\n",
    "#         total_val_loss = 0.0\n",
    "#         with torch.no_grad():\n",
    "#             for batch in val_loader:\n",
    "#                 x, y = [b.to(device) for b in batch]\n",
    "#                 outputs = model(x)\n",
    "#                 preds = torch.cat([outputs[target] for target in model.target_vars], dim=1)\n",
    "#                 loss = loss_fn(preds, y)\n",
    "#                 total_val_loss += loss.item()\n",
    "#         avg_val_loss = total_val_loss / len(val_loader)\n",
    "        \n",
    "#         is_best_val_loss = avg_val_loss < best_val_loss\n",
    "#         if is_best_val_loss:\n",
    "#             best_val_loss = avg_val_loss  \n",
    "        \n",
    "#         print(f\"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {avg_train_loss:.4f} - Val Loss: {avg_val_loss:.4f}\")\n",
    "#         overall_loss = avg_train_loss * 0.5 + avg_val_loss * 0.5\n",
    "#         # if is_best_train_loss and is_best_val_loss:\n",
    "#         if overall_loss < best_overall_loss:\n",
    "#             best_overall_loss = overall_loss\n",
    "#             best_model_state = model.state_dict()\n",
    "#             print (f\"**************** Flagged for Saving! ******************\\n\")\n",
    "\n",
    "#     print(\"Training completed!\")\n",
    "#     torch.save(best_model_state, model_path)\n",
    "#     print(f\"Best model saved to: {model_path}\")\n",
    "\n",
    "# # -------------------------------------------------------------------\n",
    "# # Instantiate and Train the Model\n",
    "# # -------------------------------------------------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define TCN hyperparameters\n",
    "kernel_size = 3\n",
    "num_layers = 5\n",
    "\n",
    "model = TCNMultiTarget(\n",
    "    all_vars=all_vars,\n",
    "    target_vars=target_vars,\n",
    "    kernel_size=kernel_size,\n",
    "    num_layers=num_layers,\n",
    "    num_channels=[32, 32, 32, 64, 64], # 128], # 128, 128], \n",
    "    custom_dilations=[8, 16, 33, 66, 132], # effective RF = 511\n",
    "    # custom_dilations=[7, 14, 29, 58, 116], # RF = 449\n",
    "    dropout=0.2,\n",
    ")\n",
    "\n",
    "# batch_size = 32\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "# val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "# # Specify model save path (ensure model_dir is defined appropriately)\n",
    "# model_path = os.path.join(model_dir, f'{group_names[group_idx]}_standard_rf{max_window}_state_dict.pt')\n",
    "\n",
    "# train_model(model, train_loader, val_loader, model_path=model_path, \n",
    "#             num_epochs=150, lr=1.5e-4, weight_decay=5e-4, device=device)\n",
    "\n",
    "# # min-max: train loss: 0.002-0.005, val loss: 0.003-0.007\n",
    "# # standard-scaler: train: 0.05 - 0.2, val: 0.1-0.3\n",
    "# print(\"Training finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe0b557",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b0e731",
   "metadata": {
    "papermill": {
     "duration": 0.006436,
     "end_time": "2025-03-04T09:48:12.413147",
     "exception": false,
     "start_time": "2025-03-04T09:48:12.406711",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9058e58f",
   "metadata": {
    "papermill": {
     "duration": 0.006284,
     "end_time": "2025-03-04T09:48:12.452984",
     "exception": false,
     "start_time": "2025-03-04T09:48:12.446700",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207d7a09",
   "metadata": {
    "papermill": {
     "duration": 0.006301,
     "end_time": "2025-03-04T09:48:12.465597",
     "exception": false,
     "start_time": "2025-03-04T09:48:12.459296",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### _Load TCNModel_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "599796be",
   "metadata": {
    "papermill": {
     "duration": 0.012009,
     "end_time": "2025-03-04T09:48:12.483953",
     "exception": false,
     "start_time": "2025-03-04T09:48:12.471944",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "state_dict_path = os.path.join(cwd[:cwd.find('IFG_DFO/RL')], \n",
    "                               'IFG_DFO/RL/models/ifg_v5/multi_trgt_tcn_models',\n",
    "                               f'{group_names[group_idx]}_standard_rf510_state_dict.pt')\n",
    "\n",
    "model_path = state_dict_path\n",
    "\n",
    "model_state_dict = torch.load(model_path, weights_only=True)\n",
    "model.load_state_dict(model_state_dict)\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "train_data = rl_setup_info['data'][rl_setup_info['ctrl_vars']+rl_setup_info['state_vars']]\n",
    "\n",
    "seq_len = rl_setup_info['seq_len']\n",
    "buffer = deque(maxlen=seq_len)\n",
    "\n",
    "for idx, row in train_data.iterrows():\n",
    "    if idx < seq_len-1:\n",
    "        buffer.append(row)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "preds_list = []\n",
    "for idx, row in train_data[seq_len-1:].iterrows():\n",
    "    buffer.append(row)\n",
    "    # Build the TCN input from the buffer\n",
    "    seq_data = np.array([row for row in buffer], dtype=np.float32)\n",
    "\n",
    "    seq_data = np.expand_dims(seq_data.T, axis=0)\n",
    "    # seq_data, _ = minmax_scale_features(seq_data, scaler=rl_setup_info['ftr_scaler'])\n",
    "    seq_data, _ = standard_scale_features(seq_data, scaler=rl_setup_info['ftr_scaler'])    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        inp_tensor = torch.tensor(seq_data, dtype=torch.float32, device=device)\n",
    "        pred_dict = model(inp_tensor)\n",
    "        \n",
    "    # # Reassemble predictions into a vector following the order in state_vars.\n",
    "    # var_names = []\n",
    "    preds = []\n",
    "    for var in rl_setup_info['trgt_vars']:\n",
    "        # var_names.append(var)\n",
    "        preds.append(pred_dict[var].item())\n",
    "        \n",
    "    preds = np.array(preds, dtype=np.float32)\n",
    "    # Convert normalized value back to original scales.\n",
    "    preds = rl_setup_info['trgt_scaler'].inverse_transform(preds.reshape(1, -1)).flatten()\n",
    "    preds_list.append(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ef0343d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target group name: hist_p3\n",
      "\n",
      "Dryer/data/vibe_temp_f\n",
      "25%: 72.17394828796387\n",
      "50%: 136.8531036376953\n",
      "75%: 171.08401107788086\n",
      "95%: 207.22205352783203\n",
      "\n",
      "Dryer/data/vibe_exhaust_temp_f\n",
      "25%: 223.31259155273438\n",
      "50%: 269.6506042480469\n",
      "75%: 332.33924102783203\n",
      "95%: 370.73231506347656\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"target group name: {group_names[group_idx]}\\n\")\n",
    "\n",
    "for idx, name in enumerate(target_vars):\n",
    "    print(f\"{name}\")\n",
    "    preds = [preds_tuple[idx] for preds_tuple in preds_list]\n",
    "    for prct in [25, 50, 75, 95]:\n",
    "        print(f\"{prct}%: {np.percentile(preds, prct)}\")\n",
    "    print (f\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1d7c8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3345e34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0391b9fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1390.597478,
   "end_time": "2025-03-04T09:48:14.955059",
   "environment_variables": {},
   "exception": null,
   "input_path": "3b_zscore_ntrgt_tcn_2sec.ipynb",
   "output_path": "papermill/hist_p2_zscore_ntrgt_tcn_2sec.ipynb",
   "parameters": {},
   "start_time": "2025-03-04T09:25:04.357581",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
