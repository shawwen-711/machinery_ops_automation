{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42bc305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install sb3-contrib\n",
    "# !pip install gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10eb9f3e",
   "metadata": {},
   "source": [
    "### _Load Train Data_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52d7160",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from collections import deque\n",
    "from matplotlib import pyplot as plt \n",
    "\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import LambdaLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9dacaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "print(cwd)\n",
    "sys.path.append(os.path.join(cwd, 'utils'))\n",
    "sys.path.append(os.path.join(cwd, 'biomass_dryer_env'))\n",
    "\n",
    "# pd.set_option('display.max_rows', None)\n",
    "from utils import tools\n",
    "from biomass_dryer_env import BiomassDryerEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5826871",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "# Check if CUDA (GPU support) is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available!\")\n",
    "    print(\"Number of GPUs:\", torch.cuda.device_count())\n",
    "    print(\"Current GPU:\", torch.cuda.current_device())\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
    "    device = 'cuda'\n",
    "    \n",
    "device = torch.device(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ffe86d",
   "metadata": {},
   "source": [
    "#### _Prepare Setup Info_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c579f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve train data from ifg_v5/train/v7\n",
    "ifg_version = 5\n",
    "train_version = 7\n",
    "\n",
    "ifg_data_dir = tools.get_ifg_dir(ifg_version)\n",
    "ifg_data_dir = os.path.join(ifg_data_dir, f\"train/v{train_version}/\")\n",
    "pkl_fps = [os.path.join(ifg_data_dir, fn) for fn in os.listdir(ifg_data_dir) if fn.endswith('.pkl') and not fn.startswith('.')]\n",
    "\n",
    "state_dict_dir = os.path.join(cwd[:cwd.find('IFG_DFO/RL')], 'IFG_DFO/RL/models/ifg_v5/multi_trgt_tcn_models')\n",
    "\n",
    "model_fps = [os.path.join(state_dict_dir, fn) for fn in os.listdir(state_dict_dir) if fn.endswith('.pt')]\n",
    "\n",
    "assert len(pkl_fps)==7, \"The number of TCN setup files is incorrect!  There should only be 7.\"\n",
    "assert len(model_fps)==7, \"The number of TCN models is incorrect!  There should only be 7.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb19c9a6-68ea-4a04-b684-0eddd8b4c067",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fps = [f\"../{fp[fp.find('RL/models/ifg_v5/multi_trgt_tcn_models'):]}\" for fp in model_fps ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99611bf-0776-4a74-9301-f88849a79f4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd0af31",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = []\n",
    "seq_len = []\n",
    "ctrl_vars = []\n",
    "state_vars = []\n",
    "\n",
    "trgt_group_names = ['hist_p1', 'hist_p2', 'hist_p3', 'hist_p4', 'wip_p1', 'wip_p2', 'wip_p3']\n",
    "trgt_vars = {}\n",
    "ftr_scalers = {}\n",
    "trgt_scalers = {}\n",
    "state_dict_paths = {}\n",
    "\n",
    "for grp_name in trgt_group_names:\n",
    "    matched_fp = [fp for fp in pkl_fps if grp_name in fp]\n",
    "    assert len(matched_fp) == 1, \"More than one matche!\"\n",
    "    fp = matched_fp[0]\n",
    "    \n",
    "    assert os.path.isfile(fp), \"Invalid file path!\"\n",
    "    with open(fp, 'rb') as file:\n",
    "        model_info = pickle.load(file)\n",
    "\n",
    "    data_df.append(model_info['data'])\n",
    "    seq_len.append(model_info['seq_len'])\n",
    "    ctrl_vars.append(model_info['ctrl_vars'])\n",
    "            \n",
    "    trgt_vars[grp_name] = model_info['trgt_vars']\n",
    "    state_vars += trgt_vars[grp_name]\n",
    "    ftr_scalers[grp_name] = model_info['ftr_scaler']\n",
    "    trgt_scalers[grp_name] = model_info['trgt_scaler']\n",
    "\n",
    "    matched_fp = [fp for fp in model_fps if grp_name in fp]\n",
    "    assert len(matched_fp) == 1, \"More than one matche!\"\n",
    "    fp = matched_fp[0]\n",
    "    # assert os.path.isfile(fp), \"Invalid file path!\"\n",
    "    state_dict_paths[grp_name] = fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565ff892",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_sublists_identical (list_of_sublists):\n",
    "    return all(sub == list_of_sublists[0] for sub in list_of_sublists)\n",
    "\n",
    "def all_dataframes_equal(dfs):\n",
    "    if not dfs:\n",
    "        return True  # or False depending on how you want to treat an empty list\n",
    "    first_df = dfs[0]\n",
    "    return all(first_df.equals(df) for df in dfs[1:])    \n",
    "\n",
    "seq_len = seq_len[0] if all_sublists_identical(seq_len) else None\n",
    "ctrl_vars = ctrl_vars[0] if all_sublists_identical(ctrl_vars) else None\n",
    "data_df = data_df[0] if all_dataframes_equal(data_df) else None\n",
    "\n",
    "assert seq_len or ctrl_vars or data_df, \"seq_len or ctrl_vars or data contains no value!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad36cb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_info = {\n",
    "    'data': data_df,\n",
    "    'seq_len': seq_len,\n",
    "    'ctrl_vars': ctrl_vars,\n",
    "    'state_vars': state_vars,\n",
    "    'trgt_group_names': trgt_group_names,\n",
    "    'trgt_vars': trgt_vars, \n",
    "    'ftr_scalers': ftr_scalers,\n",
    "    'trgt_scalers': trgt_scalers,\n",
    "    'state_dict_paths': state_dict_paths}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb076c0-40e2-4739-b1d1-8fd7a505d72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('biomass_dryer_env_setup_info.pkl', 'wb') as file:\n",
    "#     pickle.dump(setup_info, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd051d2-ce5d-4708-8979-3fee8f2f0eeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4fbe99-f87f-43aa-a340-0ac62c70c272",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2696d2f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54fc1ae0",
   "metadata": {},
   "source": [
    "### _Train RecurrentPPO_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d6370a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sb3_contrib import RecurrentPPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35b360c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_timesteps_per_epoch = setup_info['seq_len']  * 10\n",
    "\n",
    "biomass_dryer_env = make_vec_env(\n",
    "    lambda: BiomassDryerEnv(\n",
    "        setup_info=setup_info,\n",
    "        start_idx=0,\n",
    "        real_time_mode=False,\n",
    "        time_step_sec=2.0,\n",
    "        max_timesteps = max_timesteps_per_epoch\n",
    "    ),\n",
    "    n_envs=1 # n_envs=1 is common for recurrent policies\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b928af8a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# rnn_ppo_model = RecurrentPPO(\n",
    "#     policy=\"MlpLstmPolicy\",\n",
    "#     env=biomass_dryer_env,\n",
    "#     # Increase `ent_coef` to encourage broader exploration\n",
    "#     ent_coef=0.01,                # Default is often 0.0; try 0.01 or higher\n",
    "\n",
    "#     # Set the initial log std dev for the policy (controls how \"wide\" actions are at the start).\n",
    "#     # log_std_init=0.0 -> std = exp(0.0) = 1.0\n",
    "#     # log_std_init=1.0 -> std = exp(1.0) ~ 2.718\n",
    "#     # log_std_init=-0.5 -> std = exp(-0.5) ~ 0.61 (less exploration)\n",
    "#     policy_kwargs={\"log_std_init\": 0.3},  \n",
    "#     n_steps=256,              # how often the agent updates the policy\n",
    "#     batch_size=64,            # larger batch size for more stable gradient updates\n",
    "#     learning_rate=2.5e-4,\n",
    "#     gamma=0.99,\n",
    "#     verbose=0,\n",
    "#     device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# )\n",
    "\n",
    "# # from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# # class SetStartIndexCallback(BaseCallback):\n",
    "# #     def __init__(self, start_idx, verbose=0):\n",
    "# #         super().__init__(verbose)\n",
    "# #         self.start_idx = start_idx\n",
    "\n",
    "# #     def _on_rollout_start(self) -> None:\n",
    "# #         # Before collecting a rollout, reset all environments with the new start index.\n",
    "# #         self.model.env.env_method('reset', options={\"train_start_idx\": self.start_idx})\n",
    "    \n",
    "# #     def _on_step(self) -> bool:\n",
    "# #         # This callback doesn't need to intervene at each step, so simply return True.\n",
    "# #         return True\n",
    "\n",
    "# # train_start_indexes = [10, 1000, 3000]\n",
    "\n",
    "# # for idx in train_start_indexes:\n",
    "# #     callback = SetStartIndexCallback(start_idx=idx)\n",
    "# #     rnn_ppo_model.learn(total_timesteps=setup_info['seq_len'] * 25, \n",
    "# #                         reset_num_timesteps=False, \n",
    "# #                         callback=callback)\n",
    "\n",
    "# total_epochs = 10\n",
    "# for epoch in range(total_epochs):\n",
    "#     print(f\"\\nEpoch {epoch+1}/{total_epochs}\")\n",
    "#     rnn_ppo_model.learn(total_timesteps = max_timesteps_per_epoch,\n",
    "#                         reset_num_timesteps = False)\n",
    "\n",
    "# rnn_ppo_model.save(f\"tcn_rnn_ppo_bat64_lr2.5e-4_{total_epochs}epochs_model.zip\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a8c3c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ecf8b610",
   "metadata": {
    "papermill": {
     "duration": 0.002338,
     "end_time": "2025-03-06T07:41:35.379793",
     "exception": false,
     "start_time": "2025-03-06T07:41:35.377455",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### _Regular Train_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6afdcb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.006226,
     "end_time": "2025-03-06T07:41:35.370271",
     "exception": false,
     "start_time": "2025-03-06T07:41:35.364045",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# rnn_ppo_model = RecurrentPPO(\n",
    "#     policy=\"MlpLstmPolicy\",\n",
    "#     env=biomass_dryer_env,\n",
    "#     # Increase `ent_coef` to encourage broader exploration\n",
    "#     ent_coef=0.01,                # Default is often 0.0; try 0.01 or higher\n",
    "\n",
    "#     # Set the initial log std dev for the policy (controls how \"wide\" actions are at the start).\n",
    "#     # log_std_init=0.0 -> std = exp(0.0) = 1.0\n",
    "#     # log_std_init=1.0 -> std = exp(1.0) ~ 2.718\n",
    "#     # log_std_init=-0.5 -> std = exp(-0.5) ~ 0.61 (less exploration)\n",
    "#     policy_kwargs={\"log_std_init\": 0.3},  \n",
    "#     n_steps=256,              # how often the agent updates the policy\n",
    "#     batch_size=64,            # larger batch size for more stable gradient updates\n",
    "#     learning_rate=2.5e-4,\n",
    "#     gamma=0.99,\n",
    "#     verbose=0,\n",
    "#     device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# )\n",
    "\n",
    "# total_epochs = 10\n",
    "# for epoch in range(total_epochs):\n",
    "#     print(f\"\\nEpoch {epoch+1}/{total_epochs}\")\n",
    "#     rnn_ppo_model.learn(total_timesteps = max_timesteps_per_epoch,\n",
    "#                         reset_num_timesteps = False)\n",
    "\n",
    "# rnn_ppo_model.save(f\"tcn_rnn_ppo_bat64_lr2.5e-4_{total_epochs}epochs_model.zip\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69eec545",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34fec6aa",
   "metadata": {},
   "source": [
    "#### _Single-Run Annealed Train_\n",
    "    - start with wide exploration -> finish tight around the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1a59db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define total number of epochs\n",
    "train_epochs = 100\n",
    "\n",
    "# Define the initial and final values for annealing\n",
    "initial_ent_coef = 0.01\n",
    "final_ent_coef = 0.0\n",
    "initial_log_std_init = 0.2\n",
    "final_log_std_init = 0.1\n",
    "\n",
    "# Create the model with the initial settings\n",
    "rnn_ppo_model = RecurrentPPO(\n",
    "    policy=\"MlpLstmPolicy\",\n",
    "    env=biomass_dryer_env,\n",
    "    ent_coef=initial_ent_coef,  # Start with higher exploration\n",
    "    policy_kwargs={\"log_std_init\": initial_log_std_init},\n",
    "    n_steps=512, #setup_info['seq_len'],\n",
    "    batch_size=64,\n",
    "    learning_rate=1.5e-4,\n",
    "    gamma=0.99,\n",
    "    verbose=0,\n",
    "    device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# # stablizing the moisture predictions around the target output mc\n",
    "# torch.nn.utils.clip_grad_norm_(rnn_ppo_model.policy.parameters(), max_norm=0.5)\n",
    "\n",
    "# Learning rate scheduler to gradually decay the learning rate\n",
    "def lr_lambda(epoch):\n",
    "    return max(0.01, 0.95 ** epoch)  # Decrease learning rate over epochs\n",
    "\n",
    "scheduler = LambdaLR(rnn_ppo_model.policy.optimizer, lr_lambda)\n",
    "\n",
    "# Training loop with annealing of exploration parameters\n",
    "for epoch in range(train_epochs):\n",
    "    # Fraction of progress through training\n",
    "    fraction = epoch / (train_epochs - 1)\n",
    "\n",
    "    # Linearly interpolate ent_coef and log_std_init\n",
    "    current_ent_coef = initial_ent_coef + fraction * (final_ent_coef - initial_ent_coef)\n",
    "    current_log_std_init = initial_log_std_init + fraction * (final_log_std_init - initial_log_std_init)\n",
    "\n",
    "    # Update the model's exploration parameters\n",
    "    rnn_ppo_model.ent_coef = current_ent_coef\n",
    "    rnn_ppo_model.policy.log_std_init = current_log_std_init\n",
    "\n",
    "    print(f\"\\nEpoch {epoch+1}/{train_epochs}\")\n",
    "    print(f\"  Current ent_coef     = {rnn_ppo_model.ent_coef}\")\n",
    "    print(f\"  Current log_std_init = {rnn_ppo_model.policy.log_std_init}\")\n",
    "\n",
    "    # Perform one epoch of training\n",
    "    rnn_ppo_model.learn(\n",
    "        total_timesteps=max_timesteps_per_epoch,\n",
    "        reset_num_timesteps=False\n",
    "    )\n",
    "    \n",
    "    # Step the learning rate scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "    # Save model every 100 epochs\n",
    "    if (epoch+1) % train_epochs == 0:\n",
    "        rnn_ppo_model.save(f\"annealed_ntrgt_tcn_rnn_ppo_nsteps512_bat64_lr1.5e-4_{epoch+1}epochs_model.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2f2dae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c35e0d46",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad776ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "biomass_dryer_env = make_vec_env(\n",
    "    lambda: BiomassDryerEnv(\n",
    "        setup_info=setup_info,\n",
    "        start_idx=100,\n",
    "        real_time_mode=False,\n",
    "        time_step_sec=2.0,\n",
    "        max_timesteps = max_timesteps_per_epoch\n",
    "    ),\n",
    "    n_envs=1 # n_envs=1 is common for recurrent policies\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f95ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating trained RL\n",
    "obs = biomass_dryer_env.reset()\n",
    "total_rewards = []\n",
    "all_obs = []\n",
    "\n",
    "for i in range(5000):\n",
    "    action, _states = rnn_ppo_model.predict(obs)\n",
    "    obs, rewards, done, info = biomass_dryer_env.step(action)\n",
    "    all_obs.append(obs)\n",
    "    \n",
    "    total_rewards += [rewards]\n",
    "    # print(f\"episode_{i+1} total rewards: {total_rewards}\")\n",
    "\n",
    "print(f\"Average total rewards: {np.median(total_rewards):.2f}\")\n",
    "print(f\"Average output MC: {np.median([obs[0][0] for obs in all_obs]):.2f}\")        \n",
    "\n",
    "\n",
    "# Calculate the histogram data using numpy.histogram()\n",
    "hist, bin_edges = np.histogram([obs[0][0] for obs in all_obs], bins=50)\n",
    "\n",
    "# Plot the histogram using Matplotlib\n",
    "plt.bar(bin_edges[:-1], hist, width=np.diff(bin_edges), align='edge')\n",
    "plt.xlabel('Output Material MC')\n",
    "plt.ylabel('Occurrence')\n",
    "plt.title('Histogram of Output MC Values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47936d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d8ae64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653b5519",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
